{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1855a8ed",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "This notebook gives a framework to train and evaluate a model. You can copy it and adapt it to your needs.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfbfc3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ef851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression  # Example - uncomment and modify as needed\n",
    "# from sklearn.ensemble import RandomForestClassifier  # Example\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "import constants.constants as cst\n",
    "from src.preprocessing import preprocess_data\n",
    "from src.utils.calibration import plot_calibration_curve\n",
    "from src.utils.compute_metrics import compute_and_store_metrics\n",
    "from src.utils.confusion_matrix import plot_confusion_matrix\n",
    "from src.utils.load_data import load_data\n",
    "from src.utils.model_utils import save_model\n",
    "from src.utils.plot_roc import plot_roc_curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f3952",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "X = data.drop(columns=cst.TARGET)\n",
    "y = data[cst.TARGET]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270a7ba",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247763d2",
   "metadata": {},
   "source": [
    "## Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3099ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f406169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-25 16:10:15.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.preprocessing\u001b[0m:\u001b[36mpreprocess_data\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mNo preprocessor provided. Creating a new one.\u001b[0m\n",
      "\u001b[32m2025-10-25 16:10:15.865\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.preprocessing\u001b[0m:\u001b[36mpreprocess_data\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1mFitting and transforming data.\u001b[0m\n",
      "\u001b[32m2025-10-25 16:10:15.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.model_utils\u001b[0m:\u001b[36msave_model\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mModel saved to: /home/augustin/projects/xhec_dsb/05_intro_to_finance_for_data_scientists/models/preprocessor.pkl\u001b[0m\n",
      "\u001b[32m2025-10-25 16:10:15.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.preprocessing\u001b[0m:\u001b[36mpreprocess_data\u001b[0m:\u001b[36m74\u001b[0m - \u001b[1mPreprocessor fitted and saved.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train_data, y_train, preprocessor = preprocess_data(train_data, fit=True)\n",
    "\n",
    "preprocessed_test_data, y_test, _ = preprocess_data(\n",
    "    test_data, preprocessor=preprocessor, fit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4421fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model - REPLACE WITH YOUR MODEL\n",
    "# Example: model = LogisticRegression(random_state=42)\n",
    "# Example: model = RandomForestClassifier(random_state=42)\n",
    "model_baseline = ...  # <-- Replace this with your actual model\n",
    "\n",
    "# CRITICAL: Use preprocessed data for training!\n",
    "model_baseline.fit(preprocessed_train_data, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba_baseline = model_baseline.predict_proba(preprocessed_test_data)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_pred_baseline = (y_pred_proba_baseline >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred_baseline, cmap=\"Blues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e938897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if model supports it)\n",
    "if hasattr(model_baseline, \"feature_importances_\"):\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    importance_df = pd.DataFrame(\n",
    "        {\"feature\": feature_names, \"importance\": model_baseline.feature_importances_}\n",
    "    ).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df.head(15)[\"feature\"], importance_df.head(15)[\"importance\"])\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(\"Top 15 Feature Importances\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif hasattr(model_baseline, \"coef_\"):\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    importance_df = pd.DataFrame(\n",
    "        {\"feature\": feature_names, \"coefficient\": np.abs(model_baseline.coef_[0])}\n",
    "    ).sort_values(\"coefficient\", ascending=False)\n",
    "\n",
    "    print(\"\\nTop 10 Most Important Features (by coefficient magnitude):\")\n",
    "    print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e19302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation score\n",
    "cv_scores = cross_val_score(\n",
    "    model_baseline, preprocessed_train_data, y_train, cv=5, scoring=\"f1\"\n",
    ")\n",
    "print(f\"Cross-validation F1 scores: {cv_scores}\")\n",
    "print(f\"Mean CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d2b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plot_roc_curve(y_test, y_pred_proba_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(y_test, y_pred_proba_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5757fb",
   "metadata": {},
   "source": [
    "## Hyperparameter fine-tuning (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94002b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_splits = 5\n",
    "n_trials = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86629e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for hyperparameter optimization with CV.\n",
    "    Adapt the hyperparameters based on your model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Example for RandomForestClassifier - MODIFY FOR YOUR MODEL\n",
    "    # n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "    # max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    # min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "\n",
    "    # Example for LogisticRegression - MODIFY FOR YOUR MODEL\n",
    "    # C = trial.suggest_float('C', 0.001, 100.0, log=True)\n",
    "    # penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n",
    "    # solver = 'liblinear'  # Required for l1 penalty\n",
    "\n",
    "    # Suggest hyperparameters for YOUR model here\n",
    "    # param1 = trial.suggest_...\n",
    "    # param2 = trial.suggest_...\n",
    "\n",
    "    # Create model with suggested hyperparameters\n",
    "    # Example: model = RandomForestClassifier(\n",
    "    #     n_estimators=n_estimators,\n",
    "    #     max_depth=max_depth,\n",
    "    #     min_samples_split=min_samples_split,\n",
    "    #     random_state=42\n",
    "    # )\n",
    "\n",
    "    model = ...  # <-- Replace with your model using suggested hyperparameters\n",
    "\n",
    "    # Use cross-validation on TRAINING data only (more robust than single split)\n",
    "    cv_scores = cross_val_score(\n",
    "        model,\n",
    "        preprocessed_train_data,\n",
    "        y_train,\n",
    "        cv=cv_splits,  # 5-fold cross-validation\n",
    "        scoring=\"f1\",  # or 'roc_auc', 'accuracy', etc.\n",
    "        n_jobs=-1,  # Use all CPU cores\n",
    "    )\n",
    "\n",
    "    # Return mean CV score\n",
    "    return cv_scores.mean()\n",
    "\n",
    "\n",
    "# Create and run the study\n",
    "study = optuna.create_study(direction=\"maximize\")  # Maximize F1-score\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Best CV F1-Score: {study.best_value:.4f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f2c54",
   "metadata": {},
   "source": [
    "### Evaluate Best Model on Test Set\n",
    "\n",
    "Now we evaluate the best hyperparameters found by Optuna on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579133bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model with best parameters on full training set\n",
    "best_params = study.best_params.copy()\n",
    "\n",
    "# Example: model = RandomForestClassifier(**best_params, random_state=42)\n",
    "# Example: model = LogisticRegression(**best_params, random_state=42)\n",
    "model_optimized = ...  # <-- Replace with your model using **best_params\n",
    "\n",
    "model_optimized.fit(preprocessed_train_data, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_proba_optimized = model_optimized.predict_proba(preprocessed_test_data)[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.3, 0.7, 0.01)\n",
    "f1_scores = [\n",
    "    f1_score(y_test, (y_pred_proba_optimized >= t).astype(int)) for t in thresholds\n",
    "]\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "y_pred_optimized = (y_pred_proba_optimized >= optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"Optimal threshold on test set: {optimal_threshold:.3f}\")\n",
    "print(f\"Test F1-Score: {max(f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_test, y_pred_optimized, cmap=\"Greens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve for optimized model\n",
    "plot_roc_curve(y_test, y_pred_proba_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models side by side\n",
    "\n",
    "# Calculate ROC curves for both models\n",
    "fpr_baseline, tpr_baseline, _ = roc_curve(y_test, y_pred_proba_baseline)\n",
    "auc_baseline = roc_auc_score(y_test, y_pred_proba_baseline)\n",
    "\n",
    "fpr_optimized, tpr_optimized, _ = roc_curve(y_test, y_pred_proba_optimized)\n",
    "auc_optimized = roc_auc_score(y_test, y_pred_proba_optimized)\n",
    "\n",
    "# Plot both on the same figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    fpr_baseline,\n",
    "    tpr_baseline,\n",
    "    color=\"blue\",\n",
    "    lw=2,\n",
    "    label=f\"Baseline Model (AUC = {auc_baseline:.4f})\",\n",
    ")\n",
    "plt.plot(\n",
    "    fpr_optimized,\n",
    "    tpr_optimized,\n",
    "    color=\"green\",\n",
    "    lw=2,\n",
    "    label=f\"Optimized Model (AUC = {auc_optimized:.4f})\",\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Random Guess\")\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison: Baseline vs Optimized\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Baseline AUC: {auc_baseline:.4f}\")\n",
    "print(f\"Optimized AUC: {auc_optimized:.4f}\")\n",
    "print(\n",
    "    f\"Improvement: {auc_optimized - auc_baseline:.4f} ({((auc_optimized - auc_baseline) / auc_baseline * 100):.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e321a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(y_test, y_pred_proba_optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ceb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the optimized model\n",
    "\n",
    "model_calibrated = CalibratedClassifierCV(\n",
    "    model_optimized, method=\"isotonic\", cv=\"prefit\"\n",
    ")\n",
    "model_calibrated.fit(preprocessed_train_data, y_train)\n",
    "\n",
    "y_pred_proba_calibrated = model_calibrated.predict_proba(preprocessed_test_data)[:, 1]\n",
    "\n",
    "# Find optimal threshold on test set\n",
    "thresholds = np.arange(0, 1, 0.01)\n",
    "f1_scores = [\n",
    "    f1_score(y_test, (y_pred_proba_calibrated >= t).astype(int)) for t in thresholds\n",
    "]\n",
    "calibrated_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "y_pred_calibrated = (y_pred_proba_calibrated >= optimal_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1522375",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curve(y_test, y_pred_proba_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_and_store_metrics(y_test, y_pred_optimized, model_name=\"Model_Name_Optimized\")\n",
    "\n",
    "compute_and_store_metrics(y_test, y_pred_calibrated, model_name=\"Model_Name_Calibrated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c947a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final training on the entire dataset\n",
    "# Preprocess full dataset\n",
    "full_data = pd.concat([X, y], axis=1)\n",
    "preprocessed_full_data, y_full, final_preprocessor = preprocess_data(\n",
    "    full_data, fit=True\n",
    ")\n",
    "\n",
    "# Train final model with best hyperparameters\n",
    "# Example: final_model = RandomForestClassifier(**best_params, random_state=42)\n",
    "final_model = ...  # <-- Replace with your model using **best_params\n",
    "\n",
    "final_model.fit(preprocessed_full_data, y_full)\n",
    "\n",
    "# Calibrate final model\n",
    "model_calibrated_final = CalibratedClassifierCV(\n",
    "    final_model, method=\"isotonic\", cv=\"prefit\"\n",
    ")\n",
    "model_calibrated_final.fit(preprocessed_full_data, y_full)\n",
    "\n",
    "# Save both model and preprocessor\n",
    "save_model(model_calibrated_final, model_name=\"Model_Name_Calibrated\")\n",
    "save_model(final_preprocessor, model_name=\"Model_Name_preprocessor\")\n",
    "\n",
    "print(f\"✓ Model trained on full dataset ({len(y_full)} samples)\")\n",
    "print(f\"✓ Model saved with calibrated threshold: {calibrated_threshold:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "05-intro-to-finance-for-data-scientists",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
